{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473be03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 22:51:53 WARN Utils: Your hostname, qtt-HP-EliteBook-840-G6 resolves to a loopback address: 127.0.1.1; using 192.168.1.32 instead (on interface wlp58s0)\n",
      "22/11/18 22:51:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 22:51:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Spark book\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d13567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static = spark.read.json(\"./data/activity-data/\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1655c892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Arrival_Time', LongType(), True), StructField('Creation_Time', LongType(), True), StructField('Device', StringType(), True), StructField('Index', LongType(), True), StructField('Model', StringType(), True), StructField('User', StringType(), True), StructField('gt', StringType(), True), StructField('x', DoubleType(), True), StructField('y', DoubleType(), True), StructField('z', DoubleType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e0d1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(\"./data/activity-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912299c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5d0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b9752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 22:52:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f9a98758-6b04-4e23-99ff-520cdfb483a9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/18 22:52:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce5adafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|910858|\n",
      "|     stand|842480|\n",
      "|stairsdown|692880|\n",
      "|      walk|980872|\n",
      "|  stairsup|773870|\n",
      "|      null|773032|\n",
      "|      bike|798933|\n",
      "+----------+------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 935476|\n",
      "|     stand| 865247|\n",
      "|stairsdown| 711606|\n",
      "|      walk|1007382|\n",
      "|  stairsup| 794780|\n",
      "|      null| 793930|\n",
      "|      bike| 820526|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 960096|\n",
      "|     stand| 888015|\n",
      "|stairsdown| 730332|\n",
      "|      walk|1033892|\n",
      "|  stairsup| 815690|\n",
      "|      null| 814826|\n",
      "|      bike| 842118|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1818cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 23:04:24 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-343ae3d9-b6d8-4462-805a-4751df600eb1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/18 23:04:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    ".where(\"stairs\")\\\n",
    ".where(\"gt is not null\")\\\n",
    ".select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    ".writeStream\\\n",
    ".queryName(\"simple_transform\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"append\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171b17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 23:08:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-03346ba6-5336-400a-908c-b03619a52fe7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/18 23:08:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    ".drop(\"avg(Arrival_time)\")\\\n",
    ".drop(\"avg(Creation_Time)\")\\\n",
    ".drop(\"avg(Index)\")\\\n",
    ".writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "606bd5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|       sit|  null|-5.49433244039590...|2.791446281700068E-4|-2.33994461689890...|\n",
      "|      walk|nexus4|-0.00390116006094368|0.001052508689953...|-6.95435553042992...|\n",
      "|      walk|  null|-0.00390116006094368|0.001052508689953...|-6.95435553042992...|\n",
      "|  stairsup|  null|-0.02479965287771635|-0.00800392344379...|  -0.100340884150604|\n",
      "|     stand|  null|-3.11082189691724...|3.218461665975318E-4|2.141300040636475...|\n",
      "|      bike|  null|0.022688759550866824|-0.00877912156368...|-0.08251001663412387|\n",
      "|  stairsup|nexus4|-0.02479965287771635|-0.00800392344379...|  -0.100340884150604|\n",
      "|      null|nexus4|4.796918779024537E-4|-0.00601540958963...|-0.01013356489164804|\n",
      "|      null|  null|4.796918779024537E-4|-0.00601540958963...|-0.01013356489164804|\n",
      "|stairsdown|  null|0.021613908669165287|-0.03249018824752615| 0.12035922691504057|\n",
      "|      null|  null|-0.00847688860109...|-7.30455258739177...|0.003090601491419...|\n",
      "|       sit|nexus4|-5.49433244039590...|2.791446281700068E-4|-2.33994461689890...|\n",
      "|stairsdown|nexus4|0.021613908669165287|-0.03249018824752615| 0.12035922691504057|\n",
      "|     stand|nexus4|-3.11082189691724...|3.218461665975318E-4|2.141300040636475...|\n",
      "|      null|nexus4|-0.00847688860109...|-7.30455258739177...|0.003090601491419...|\n",
      "|      bike|nexus4|0.022688759550866824|-0.00877912156368...|-0.08251001663412387|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM device_counts\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "505b2a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 23:11:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-baa36dfa-66b4-4646-97b1-a3b4db72cd0f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/18 23:11:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 570:============================>                           (6 + 1) / 12]\r"
     ]
    }
   ],
   "source": [
    "historicalAgg = static.groupBy(\"gt\", \"model\").avg()\n",
    "deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
    ".cube(\"gt\", \"model\").avg()\\\n",
    ".join(historicalAgg, [\"gt\", \"model\"])\\\n",
    ".writeStream.queryName(\"device_count\").format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20819819",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": " Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhost1:port1,host2:port2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/streaming.py:469\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m:  Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 572:==========>     (8 + 1) / 12][Stage 573:>                (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    ".option(\"subscribe\", \"topic1\")\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "351526d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 590:>(11 + 1) / 12][Stage 591:>  (0 + 0) / 1][Stage 594:>  (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|   avg(Arrival_Time)|  avg(Creation_Time)|        avg(Index)|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|      bike|nexus4| 0.02286620854109702|-0.00976932276685...|-0.08249503213481174|1.424751134339986...|1.424752127369588...| 326459.6867328154| 0.02268875955086685|-0.00877912156368...|-0.08251001663412341|\n",
      "|      null|nexus4|-0.00763194754145...|-1.27732722151030...|0.002400666320012182|1.424749002876339...|1.424749919482128...| 219276.9663669269|-0.00847688860109...|-7.30455258739188...|0.003090601491419...|\n",
      "|stairsdown|nexus4|0.021880959536468035| -0.0327547414497682| 0.12003916598287755|1.424744591412857E12|1.424745503635638...|230452.44623187225| 0.02161390866916542|-0.03249018824752617| 0.12035922691504071|\n",
      "|     stand|nexus4|-2.52219618272280...|2.343823197296144E-4|1.612846683698812...|1.424743637921210...|1.424744579547462...|31317.877585550017|-3.11082189691709...|3.218461665975354...|2.141300040636498E-4|\n",
      "|      walk|nexus4|-0.00457727699360...|0.001481353289135748|-6.34957090093321...|1.424746420641790...|1.424747351060674...|149760.09974990616|-0.00390116006094...|0.001052508689953...|-6.95435553042996...|\n",
      "|       sit|nexus4|-5.26663817080944...|1.744979497984139...|-2.49086195512377...|1.424741207868230...|1.424742112220355...| 74577.84690275553|-5.49433244039556...|2.791446281700041E-4|-2.33994461689905...|\n",
      "|  stairsup|nexus4|-0.02494183345828...|-0.00859683397354828|-0.10028111649418604|1.424745996101162...|1.424746915892736...|227912.96550673083|-0.02479965287771647|-0.00800392344379...|-0.10034088415060388|\n",
      "+----------+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 667:=====>          (4 + 1) / 12][Stage 668:>                (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM device_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99128161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
