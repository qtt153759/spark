{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4349b081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 07:42:32 WARN Utils: Your hostname, qtt-HP-EliteBook-840-G6 resolves to a loopback address: 127.0.1.1; using 10.13.39.128 instead (on interface wlp58s0)\n",
      "23/02/03 07:42:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/02/03 07:42:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/03 07:42:33 WARN DependencyUtils: Local jar /home/qtt/spark/jars/postgresql-42.5.0.jar does not exist, skipping.\n",
      "23/02/03 07:42:33 INFO SparkContext: Running Spark version 3.3.1\n",
      "23/02/03 07:42:34 INFO ResourceUtils: ==============================================================\n",
      "23/02/03 07:42:34 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/02/03 07:42:34 INFO ResourceUtils: ==============================================================\n",
      "23/02/03 07:42:34 INFO SparkContext: Submitted application: Word Count\n",
      "23/02/03 07:42:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/02/03 07:42:34 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/02/03 07:42:34 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/02/03 07:42:34 INFO SecurityManager: Changing view acls to: qtt\n",
      "23/02/03 07:42:34 INFO SecurityManager: Changing modify acls to: qtt\n",
      "23/02/03 07:42:34 INFO SecurityManager: Changing view acls groups to: \n",
      "23/02/03 07:42:34 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/02/03 07:42:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(qtt); groups with view permissions: Set(); users  with modify permissions: Set(qtt); groups with modify permissions: Set()\n",
      "23/02/03 07:42:34 INFO Utils: Successfully started service 'sparkDriver' on port 39529.\n",
      "23/02/03 07:42:34 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/02/03 07:42:34 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/02/03 07:42:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/02/03 07:42:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/02/03 07:42:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/02/03 07:42:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f86f4249-ab1f-4720-830b-55fba6028647\n",
      "23/02/03 07:42:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/02/03 07:42:34 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/02/03 07:42:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/02/03 07:42:34 ERROR SparkContext: Failed to add ../jars/postgresql-42.5.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/qtt/spark/jars/postgresql-42.5.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/03 07:42:35 INFO Executor: Starting executor ID driver on host 10.13.39.128\n",
      "23/02/03 07:42:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/02/03 07:42:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37297.\n",
      "23/02/03 07:42:35 INFO NettyBlockTransferService: Server created on 10.13.39.128:37297\n",
      "23/02/03 07:42:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/02/03 07:42:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.13.39.128, 37297, None)\n",
      "23/02/03 07:42:35 INFO BlockManagerMasterEndpoint: Registering block manager 10.13.39.128:37297 with 434.4 MiB RAM, BlockManagerId(driver, 10.13.39.128, 37297, None)\n",
      "23/02/03 07:42:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.13.39.128, 37297, None)\n",
      "23/02/03 07:42:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.13.39.128, 37297, None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\")\\\n",
    ".config(\"spark.jars\", \"../jars/postgresql-42.5.0.jar\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087b98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.postgresql.Driver\"\n",
    "path = \"//localhost:5432/demo\"\n",
    "url = \"jdbc:postgresql:\" + path\n",
    "tablename = \"bookings\"\n",
    "username=\"qtt\"\n",
    "password=\"truong157359\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d24634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/03 07:42:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/02/03 07:42:40 INFO SharedState: Warehouse path is 'file:/home/qtt/spark/pyspark_template/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\\\n",
    ".option(\"dbtable\", tablename).option(\"driver\", driver)\\\n",
    ".option(\"user\", username).option(\"password\", password).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743ee5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/10 20:19:04 INFO CodeGenerator: Code generated in 161.174492 ms\n",
      "22/11/10 20:19:04 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "22/11/10 20:19:04 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/11/10 20:19:04 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "22/11/10 20:19:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/11/10 20:19:04 INFO DAGScheduler: Missing parents: List()\n",
      "22/11/10 20:19:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/11/10 20:19:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.6 KiB, free 434.4 MiB)\n",
      "22/11/10 20:19:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.4 MiB)\n",
      "22/11/10 20:19:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.173:37545 (size: 6.5 KiB, free: 434.4 MiB)\n",
      "22/11/10 20:19:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "22/11/10 20:19:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/11/10 20:19:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/11/10 20:19:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.173, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/11/10 20:19:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/10 20:19:07 INFO JDBCRDD: closed connection\n",
      "22/11/10 20:19:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2063 bytes result sent to driver\n",
      "22/11/10 20:19:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2551 ms on 192.168.1.173 (executor driver) (1/1)\n",
      "22/11/10 20:19:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/11/10 20:19:07 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.844 s\n",
      "22/11/10 20:19:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/11/10 20:19:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/11/10 20:19:07 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.901095 s\n",
      "22/11/10 20:19:07 INFO CodeGenerator: Code generated in 32.780424 ms\n",
      "+--------+-------------------+------------+\n",
      "|book_ref|          book_date|total_amount|\n",
      "+--------+-------------------+------------+\n",
      "|  000004|2016-08-13 19:40:00|    55800.00|\n",
      "|  00000F|2017-07-05 07:12:00|   265700.00|\n",
      "|  000010|2017-01-08 23:45:00|    50900.00|\n",
      "|  000012|2017-07-14 13:02:00|    37900.00|\n",
      "|  000026|2016-08-30 15:08:00|    95600.00|\n",
      "|  00002D|2017-05-20 22:45:00|   114700.00|\n",
      "|  000034|2016-08-08 09:46:00|    49100.00|\n",
      "|  00003F|2016-12-12 19:02:00|   109800.00|\n",
      "|  000048|2016-09-17 05:57:00|    92400.00|\n",
      "|  00004A|2016-10-14 01:57:00|    29000.00|\n",
      "|  000050|2016-09-18 04:01:00|    36200.00|\n",
      "|  000055|2017-03-09 02:18:00|    50800.00|\n",
      "|  000061|2016-11-12 01:28:00|    35600.00|\n",
      "|  000067|2016-08-12 01:36:00|   102100.00|\n",
      "|  000068|2017-08-15 18:27:00|    18100.00|\n",
      "|  00006A|2016-11-05 09:02:00|   106100.00|\n",
      "|  00006B|2016-11-29 11:59:00|   382000.00|\n",
      "|  00007A|2016-10-19 00:55:00|     8200.00|\n",
      "|  00007C|2016-09-27 03:00:00|    22600.00|\n",
      "|  00007D|2016-12-31 23:49:00|   103600.00|\n",
      "+--------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dbDataFrame.show()\n",
    "dbDataFrame.createOrReplaceTempView(\"bookings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80690604",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=spark.sql(\"Select * from bookings limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db5120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/10 20:19:54 INFO CodeGenerator: Code generated in 17.62081 ms\n",
      "22/11/10 20:19:54 INFO DAGScheduler: Registering RDD 5 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "22/11/10 20:19:54 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/11/10 20:19:54 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "22/11/10 20:19:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/11/10 20:19:54 INFO DAGScheduler: Missing parents: List()\n",
      "22/11/10 20:19:54 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/11/10 20:19:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.2 KiB, free 434.4 MiB)\n",
      "22/11/10 20:19:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 434.4 MiB)\n",
      "22/11/10 20:19:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.173:37545 (size: 7.3 KiB, free: 434.4 MiB)\n",
      "22/11/10 20:19:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "22/11/10 20:19:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/11/10 20:19:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/11/10 20:19:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.173, executor driver, partition 0, PROCESS_LOCAL, 4288 bytes) taskResourceAssignments Map()\n",
      "22/11/10 20:19:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "22/11/10 20:19:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.173:37545 in memory (size: 6.5 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/10 20:19:56 INFO JDBCRDD: closed connection\n",
      "22/11/10 20:19:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1827 bytes result sent to driver\n",
      "22/11/10 20:19:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2059 ms on 192.168.1.173 (executor driver) (1/1)\n",
      "22/11/10 20:19:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/11/10 20:19:56 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.107 s\n",
      "22/11/10 20:19:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/11/10 20:19:56 INFO DAGScheduler: running: Set()\n",
      "22/11/10 20:19:56 INFO DAGScheduler: waiting: Set()\n",
      "22/11/10 20:19:56 INFO DAGScheduler: failed: Set()\n",
      "22/11/10 20:19:56 INFO CodeGenerator: Code generated in 14.794626 ms\n",
      "22/11/10 20:19:56 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "22/11/10 20:19:56 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/11/10 20:19:56 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "22/11/10 20:19:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "22/11/10 20:19:56 INFO DAGScheduler: Missing parents: List()\n",
      "22/11/10 20:19:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/11/10 20:19:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.9 KiB, free 434.4 MiB)\n",
      "22/11/10 20:19:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.4 MiB)\n",
      "22/11/10 20:19:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.173:37545 (size: 5.8 KiB, free: 434.4 MiB)\n",
      "22/11/10 20:19:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "22/11/10 20:19:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/11/10 20:19:56 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "22/11/10 20:19:56 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (192.168.1.173, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/11/10 20:19:56 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "22/11/10 20:19:56 INFO ShuffleBlockFetcherIterator: Getting 1 (304.0 B) non-empty blocks including 1 (304.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/11/10 20:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
      "22/11/10 20:19:56 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2800 bytes result sent to driver\n",
      "22/11/10 20:19:56 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 74 ms on 192.168.1.173 (executor driver) (1/1)\n",
      "22/11/10 20:19:56 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "22/11/10 20:19:56 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.089 s\n",
      "22/11/10 20:19:56 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/11/10 20:19:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "22/11/10 20:19:56 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.105282 s\n",
      "+--------+-------------------+------------+\n",
      "|book_ref|          book_date|total_amount|\n",
      "+--------+-------------------+------------+\n",
      "|  000004|2016-08-13 19:40:00|    55800.00|\n",
      "|  00000F|2017-07-05 07:12:00|   265700.00|\n",
      "|  000010|2017-01-08 23:45:00|    50900.00|\n",
      "|  000012|2017-07-14 13:02:00|    37900.00|\n",
      "|  000026|2016-08-30 15:08:00|    95600.00|\n",
      "|  00002D|2017-05-20 22:45:00|   114700.00|\n",
      "|  000034|2016-08-08 09:46:00|    49100.00|\n",
      "|  00003F|2016-12-12 19:02:00|   109800.00|\n",
      "|  000048|2016-09-17 05:57:00|    92400.00|\n",
      "|  00004A|2016-10-14 01:57:00|    29000.00|\n",
      "+--------+-------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/10 20:48:29 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.173:37545 in memory (size: 5.8 KiB, free: 434.4 MiB)\n",
      "22/11/10 20:48:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.173:37545 in memory (size: 7.3 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "d1.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331e786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
